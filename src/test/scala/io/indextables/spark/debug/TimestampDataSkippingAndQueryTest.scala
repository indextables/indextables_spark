/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package io.indextables.spark.debug

import java.sql.Timestamp
import java.time.Instant

import org.apache.spark.sql.functions._
import org.apache.spark.sql.SaveMode

import io.indextables.spark.TestBase

/**
 * Comprehensive test to validate timestamp data skipping and query correctness for V2 DataSource.
 *
 * Tests specifically for the bug where TimestampType is not handled in:
 *   1. StatisticsCalculator.convertToString (line 51-61) 2. StatisticsCalculator.compareValues (line 63-77) 3.
 *      IndexTables4SparkScan.convertValuesForComparison (line 481-586)
 *
 * This causes:
 *   - Incorrect min/max statistics storage (toString instead of proper microseconds)
 *   - Failed data skipping (always returns 0 for comparison)
 *   - Incorrect BETWEEN query results (string comparison instead of numeric)
 */
class TimestampDataSkippingAndQueryTest extends TestBase {

  private def isNativeLibraryAvailable(): Boolean =
    try {
      import io.indextables.spark.search.TantivyNative
      TantivyNative.ensureLibraryLoaded()
    } catch {
      case _: Exception => false
    }

  test("V2 DataSource: timestamp BETWEEN query with data skipping validation") {
    assume(isNativeLibraryAvailable(), "Native Tantivy library not available - skipping integration test")

    withTempPath { tempPath =>
      println(s"ğŸ”§ Using temp path: $tempPath")

      val sparkSession = spark
      import sparkSession.implicits._

      // Create test data with three distinct timestamp ranges in separate splits
      // Split 1: Early morning (04:00 - 04:30)
      // Split 2: Target range (04:32 - 05:00) - should match BETWEEN query
      // Split 3: Later morning (05:30 - 06:00)

      val split1Data = Seq(
        (1, "split1_early", Timestamp.from(Instant.parse("2025-11-07T04:00:00.000Z"))),
        (2, "split1_early", Timestamp.from(Instant.parse("2025-11-07T04:15:30.500Z"))),
        (3, "split1_early", Timestamp.from(Instant.parse("2025-11-07T04:29:59.999Z")))
      ).toDF("id", "name", "timestamp_field")

      val split2Data = Seq(
        (4, "split2_target", Timestamp.from(Instant.parse("2025-11-07T04:32:46.402Z"))),
        (5, "split2_target", Timestamp.from(Instant.parse("2025-11-07T04:45:00.000Z"))),
        (6, "split2_target", Timestamp.from(Instant.parse("2025-11-07T05:00:01.101Z")))
      ).toDF("id", "name", "timestamp_field")

      val split3Data = Seq(
        (7, "split3_late", Timestamp.from(Instant.parse("2025-11-07T05:30:00.000Z"))),
        (8, "split3_late", Timestamp.from(Instant.parse("2025-11-07T05:45:30.123Z"))),
        (9, "split3_late", Timestamp.from(Instant.parse("2025-11-07T06:00:00.000Z")))
      ).toDF("id", "name", "timestamp_field")

      println("ğŸ“ Writing split 1 (early morning 04:00-04:30):")
      split1Data.show(false)

      println("ğŸ“ Writing split 2 (target range 04:32-05:00):")
      split2Data.show(false)

      println("ğŸ“ Writing split 3 (late morning 05:30-06:00):")
      split3Data.show(false)

      // Write each dataset separately to ensure separate split files
      // Force single partition per write to ensure we get distinct splits
      println("ğŸ’¾ Writing split 1...")
      split1Data
        .coalesce(1)
        .write
        .format("io.indextables.spark.core.IndexTables4SparkTableProvider")
        .mode(SaveMode.Overwrite)
        .save(tempPath)

      println("ğŸ’¾ Writing split 2...")
      split2Data
        .coalesce(1)
        .write
        .format("io.indextables.spark.core.IndexTables4SparkTableProvider")
        .mode(SaveMode.Append)
        .save(tempPath)

      println("ğŸ’¾ Writing split 3...")
      split3Data
        .coalesce(1)
        .write
        .format("io.indextables.spark.core.IndexTables4SparkTableProvider")
        .mode(SaveMode.Append)
        .save(tempPath)

      println("âœ… All splits written successfully")

      // Verify transaction log shows 3 split files
      val txLogPath = s"$tempPath/_transaction_log"
      println(s"ğŸ“‹ Checking transaction log at: $txLogPath")

      // Read transaction log to verify split files
      import org.apache.hadoop.fs.Path
      val fs = new Path(tempPath).getFileSystem(spark.sparkContext.hadoopConfiguration)
      val splitFiles = fs
        .listStatus(new Path(tempPath))
        .filter(_.getPath.getName.endsWith(".split"))
        .map(_.getPath.getName)
        .sorted

      println(s"ğŸ“¦ Found ${splitFiles.length} split files:")
      splitFiles.foreach(f => println(s"   - $f"))

      assert(splitFiles.length == 3, s"Expected 3 split files, found ${splitFiles.length}")

      // Note: Transaction log files are GZIP compressed by default, so we can't easily inspect them
      // as raw JSON. The actual data skipping verification happens in the BETWEEN query below.

      // Now test the BETWEEN query
      println("\nğŸ” Testing BETWEEN query...")
      val readData = spark.read
        .format("io.indextables.spark.core.IndexTables4SparkTableProvider")
        .load(tempPath)

      println("ğŸ“‹ Read schema:")
      readData.printSchema()

      // Test 1: BETWEEN query using SQL timestamp format
      println("\nğŸ§ª Test 1: BETWEEN query with SQL timestamp strings")
      // Use actual Timestamp objects to avoid timezone issues
      val betweenStart = Timestamp.from(Instant.parse("2025-11-07T04:32:46.402Z"))
      val betweenEnd   = Timestamp.from(Instant.parse("2025-11-07T05:00:01.101Z"))
      val betweenQuery = readData.filter(
        col("timestamp_field").between(betweenStart, betweenEnd)
      )

      println("Executing BETWEEN query...")
      val results = betweenQuery.collect()

      println(s"\nğŸ“Š Query returned ${results.length} rows:")
      betweenQuery.show(false)

      // Validate results
      println("\nâœ… Validating query results...")
      assert(results.length == 3, s"Expected 3 results from split2, got ${results.length}")

      val resultIds = results.map(_.getInt(0)).sorted
      assert(resultIds.sameElements(Array(4, 5, 6)), s"Expected IDs [4,5,6], got ${resultIds.mkString("[", ",", "]")}")

      val resultNames = results.map(_.getString(1)).toSet
      assert(resultNames == Set("split2_target"), s"Expected only 'split2_target' names, got $resultNames")

      println("âœ… Query results are correct!")

      // Test 2: Verify data skipping by checking execution plan
      println("\nğŸ” Test 2: Verify data skipping is working...")

      // Enable detailed logging to see data skipping in action
      spark.sparkContext.setLogLevel("DEBUG")

      // Use actual Timestamp objects to avoid timezone issues
      val explainQuery = readData.filter(
        col("timestamp_field").between(betweenStart, betweenEnd)
      )

      println("\nğŸ“‹ Query execution plan:")
      explainQuery.explain(extended = true)

      // Force execution and count
      val count = explainQuery.count()
      println(s"\nğŸ“Š Query count: $count")
      assert(count == 3, s"Expected count of 3, got $count")

      // Test 3: Additional timestamp range queries
      println("\nğŸ§ª Test 3: Additional timestamp range queries")

      // Query for split1 range only - Use Timestamp objects to avoid timezone issues
      val split1Cutoff = Timestamp.from(Instant.parse("2025-11-07T04:30:00.000Z"))
      val split1Query = readData.filter(
        col("timestamp_field") < split1Cutoff
      )
      val split1Results = split1Query.collect()
      println(s"\nSplit 1 query (< 04:30): ${split1Results.length} rows")
      split1Query.show(false)
      assert(split1Results.length == 3, s"Expected 3 rows from split1, got ${split1Results.length}")

      // Query for split3 range only - Use Timestamp objects to avoid timezone issues
      val split3Cutoff = Timestamp.from(Instant.parse("2025-11-07T05:30:00.000Z"))
      val split3Query = readData.filter(
        col("timestamp_field") > split3Cutoff
      )
      val split3Results = split3Query.collect()
      println(s"\nSplit 3 query (> 05:30): ${split3Results.length} rows")
      split3Query.show(false)
      assert(split3Results.length == 2, s"Expected 2 rows from split3, got ${split3Results.length}")

      // Test 4: ISO 8601 format with timezone
      println("\nğŸ§ª Test 4: BETWEEN query with ISO 8601 format (with timezone)")
      val iso8601Query = readData.filter(
        col("timestamp_field").between(
          "2025-11-07T04:32:46.402+00:00",
          "2025-11-07T05:00:01.101+00:00"
        )
      )
      val iso8601Results = iso8601Query.collect()
      println(s"\nISO 8601 query: ${iso8601Results.length} rows")
      iso8601Query.show(false)
      assert(iso8601Results.length == 3, s"Expected 3 rows with ISO 8601 format, got ${iso8601Results.length}")

      println("\nâœ… All timestamp tests passed!")
      println("âœ… Data skipping is working correctly!")
      println("âœ… Query results are accurate!")
    }
  }

  test("V2 DataSource: timestamp comparison operators validation") {
    assume(isNativeLibraryAvailable(), "Native Tantivy library not available - skipping integration test")

    withTempPath { tempPath =>
      println(s"ğŸ”§ Using temp path: $tempPath")

      val sparkSession = spark
      import sparkSession.implicits._

      // Create test data with various timestamps
      val testData = Seq(
        (1, Timestamp.from(Instant.parse("2025-11-07T03:00:00.000Z"))),
        (2, Timestamp.from(Instant.parse("2025-11-07T04:00:00.000Z"))),
        (3, Timestamp.from(Instant.parse("2025-11-07T05:00:00.000Z"))),
        (4, Timestamp.from(Instant.parse("2025-11-07T06:00:00.000Z"))),
        (5, Timestamp.from(Instant.parse("2025-11-07T07:00:00.000Z")))
      ).toDF("id", "ts")

      println("ğŸ“ Test data:")
      testData.show(false)

      testData
        .coalesce(1)
        .write
        .format("io.indextables.spark.core.IndexTables4SparkTableProvider")
        .mode(SaveMode.Overwrite)
        .save(tempPath)

      val readData = spark.read
        .format("io.indextables.spark.core.IndexTables4SparkTableProvider")
        .load(tempPath)

      // Test various comparison operators
      println("\nğŸ§ª Testing comparison operators...")

      // Test: = (equal) - Use actual Timestamp to avoid timezone issues
      val ts3          = Timestamp.from(Instant.parse("2025-11-07T05:00:00.000Z"))
      val equalResults = readData.filter($"ts" === ts3).collect()
      println(s"Equal (=) test: ${equalResults.length} rows")
      assert(equalResults.length == 1, s"Expected 1 row, got ${equalResults.length}")
      assert(equalResults(0).getInt(0) == 3, s"Expected id=3, got ${equalResults(0).getInt(0)}")

      // Test: > (greater than) - Use actual Timestamp to avoid timezone issues
      val gtResults = readData.filter($"ts" > ts3).collect()
      println(s"Greater than (>) test: ${gtResults.length} rows")
      assert(gtResults.length == 2, s"Expected 2 rows, got ${gtResults.length}")

      // Test: >= (greater than or equal) - Use actual Timestamp to avoid timezone issues
      val gteResults = readData.filter($"ts" >= ts3).collect()
      println(s"Greater than or equal (>=) test: ${gteResults.length} rows")
      assert(gteResults.length == 3, s"Expected 3 rows, got ${gteResults.length}")

      // Test: < (less than) - Use actual Timestamp to avoid timezone issues
      val ltResults = readData.filter($"ts" < ts3).collect()
      println(s"Less than (<) test: ${ltResults.length} rows")
      assert(ltResults.length == 2, s"Expected 2 rows, got ${ltResults.length}")

      // Test: <= (less than or equal) - Use actual Timestamp to avoid timezone issues
      val lteResults = readData.filter($"ts" <= ts3).collect()
      println(s"Less than or equal (<=) test: ${lteResults.length} rows")
      assert(lteResults.length == 3, s"Expected 3 rows, got ${lteResults.length}")

      println("\nâœ… All comparison operator tests passed!")
    }
  }

  test("V2 DataSource: timestamp microsecond precision validation") {
    assume(isNativeLibraryAvailable(), "Native Tantivy library not available - skipping integration test")

    withTempPath { tempPath =>
      println(s"ğŸ”§ Using temp path: $tempPath")

      val sparkSession = spark
      import sparkSession.implicits._

      // Test microsecond precision (Spark timestamps are microsecond precision)
      val baseTime = Instant.parse("2025-11-07T12:00:00.000000Z")
      val testData = Seq(
        (1, Timestamp.from(baseTime.plusNanos(1000))),    // +1 microsecond
        (2, Timestamp.from(baseTime.plusNanos(500000))),  // +500 microseconds
        (3, Timestamp.from(baseTime.plusNanos(1000000))), // +1000 microseconds (1 ms)
        (4, Timestamp.from(baseTime.plusNanos(2000000)))  // +2000 microseconds (2 ms)
      ).toDF("id", "ts")

      println("ğŸ“ Microsecond precision test data:")
      testData.show(false)
      testData.select($"id", $"ts", $"ts".cast("long").as("micros_since_epoch")).show(false)

      testData
        .coalesce(1)
        .write
        .format("io.indextables.spark.core.IndexTables4SparkTableProvider")
        .mode(SaveMode.Overwrite)
        .save(tempPath)

      val readData = spark.read
        .format("io.indextables.spark.core.IndexTables4SparkTableProvider")
        .load(tempPath)

      println("\nğŸ“ Read back data:")
      readData.show(false)
      readData.select($"id", $"ts", $"ts".cast("long").as("micros_since_epoch")).show(false)

      // Verify microsecond precision is preserved
      val results = readData.collect().sortBy(_.getInt(0))
      assert(results.length == 4, s"Expected 4 rows, got ${results.length}")

      // Test range query with microsecond precision
      val rangeStart = baseTime.plusNanos(250000)  // +250 microseconds
      val rangeEnd   = baseTime.plusNanos(1500000) // +1500 microseconds

      val rangeQuery = readData.filter(
        $"ts".between(
          Timestamp.from(rangeStart),
          Timestamp.from(rangeEnd)
        )
      )

      println(s"\nğŸ” Range query [+250Î¼s to +1500Î¼s]:")
      val rangeResults = rangeQuery.collect()
      rangeQuery.show(false)

      // Should return records 2 and 3
      assert(rangeResults.length == 2, s"Expected 2 rows in microsecond range, got ${rangeResults.length}")
      val rangeIds = rangeResults.map(_.getInt(0)).sorted
      assert(rangeIds.sameElements(Array(2, 3)), s"Expected ids [2,3], got ${rangeIds.mkString("[", ",", "]")}")

      println("\nâœ… Microsecond precision validation passed!")
    }
  }
}
