/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.tantivy4spark.io

import com.tantivy4spark.RealS3TestBase
import software.amazon.awssdk.services.s3.S3Client
import software.amazon.awssdk.auth.credentials.{AwsBasicCredentials, StaticCredentialsProvider}
import software.amazon.awssdk.regions.Region
import software.amazon.awssdk.services.s3.model._
import java.io.{ByteArrayInputStream, File, FileInputStream}
import java.util.Properties
import java.util.UUID
import scala.util.Using
import scala.util.Random

/**
 * Real AWS S3 multipart upload tests using test-tantivy4sparkbucket.
 *
 * Tests actual multipart upload functionality against real AWS S3:
 * - Small files using single-part upload
 * - Large files using multipart upload
 * - Custom multipart configuration
 * - Upload failure handling with retries
 * - Streaming uploads from input streams
 *
 * Credentials are loaded from ~/.aws/credentials file.
 */
class RealS3MultipartUploaderTest extends RealS3TestBase {

  private val S3_BUCKET = "test-tantivy4sparkbucket"
  private val S3_REGION = "us-east-2"

  // Generate unique test run ID to avoid conflicts
  private val testRunId = UUID.randomUUID().toString.substring(0, 8)
  private val testKeyPrefix = s"multipart-test-$testRunId"

  private var awsCredentials: Option[(String, String)] = None
  private var s3Client: Option[S3Client] = None
  private var multipartUploader: Option[S3MultipartUploader] = None

  override def beforeAll(): Unit = {
    super.beforeAll()

    // Load AWS credentials from ~/.aws/credentials
    awsCredentials = loadAwsCredentials()

    if (awsCredentials.isDefined) {
      val (accessKey, secretKey) = awsCredentials.get

      // Create S3Client for real AWS S3
      val credentials = AwsBasicCredentials.create(accessKey, secretKey)
      val s3ClientInstance = S3Client.builder()
        .region(Region.of(S3_REGION))
        .credentialsProvider(StaticCredentialsProvider.create(credentials))
        .build()

      s3Client = Some(s3ClientInstance)
      multipartUploader = Some(new S3MultipartUploader(s3ClientInstance))

      println(s"ðŸ” AWS credentials loaded successfully")
      println(s"ðŸŒŠ S3Client configured for bucket: $S3_BUCKET in region: $S3_REGION")
      println(s"ðŸ“ Test key prefix: $testKeyPrefix")
    } else {
      println(s"âš ï¸  No AWS credentials found in ~/.aws/credentials - tests will be skipped")
    }
  }

  override def afterAll(): Unit = {
    // Clean up S3Client and uploader
    multipartUploader.foreach(_.shutdown())
    s3Client.foreach(_.close())

    // Clean up test objects from S3
    if (awsCredentials.isDefined && s3Client.isDefined) {
      cleanupTestObjects()
    }

    super.afterAll()
  }

  /**
   * Load AWS credentials from ~/.aws/credentials file.
   */
  private def loadAwsCredentials(): Option[(String, String)] = {
    try {
      val home = System.getProperty("user.home")
      val credFile = new File(s"$home/.aws/credentials")

      if (!credFile.exists()) {
        println(s"âš ï¸  AWS credentials file not found: ${credFile.getPath}")
        return None
      }

      Using(new FileInputStream(credFile)) { fis =>
        val props = new Properties()
        props.load(fis)

        val accessKey = props.getProperty("aws_access_key_id")
        val secretKey = props.getProperty("aws_secret_access_key")

        if (accessKey != null && secretKey != null) {
          Some((accessKey, secretKey))
        } else {
          println(s"âš ï¸  AWS credentials not found in default profile")
          None
        }
      }.getOrElse(None)

    } catch {
      case e: Exception =>
        println(s"âš ï¸  Error loading AWS credentials: ${e.getMessage}")
        None
    }
  }

  /**
   * Clean up test objects from S3.
   */
  private def cleanupTestObjects(): Unit = {
    try {
      val client = s3Client.get

      // List all objects with our test prefix
      val listRequest = ListObjectsV2Request.builder()
        .bucket(S3_BUCKET)
        .prefix(testKeyPrefix)
        .build()

      val response = client.listObjectsV2(listRequest)
      val objects = response.contents()

      if (!objects.isEmpty) {
        println(s"ðŸ§¹ Cleaning up ${objects.size()} test objects from S3...")

        objects.forEach { obj =>
          val deleteRequest = DeleteObjectRequest.builder()
            .bucket(S3_BUCKET)
            .key(obj.key())
            .build()

          try {
            client.deleteObject(deleteRequest)
            println(s"   Deleted: s3://$S3_BUCKET/${obj.key()}")
          } catch {
            case e: Exception =>
              println(s"   Warning: Failed to delete ${obj.key()}: ${e.getMessage}")
          }
        }
      }

    } catch {
      case e: Exception =>
        println(s"âš ï¸  Warning: Could not clean up test objects: ${e.getMessage}")
    }
  }

  test("Real S3: Small files should use single-part upload") {
    assume(awsCredentials.isDefined, "AWS credentials required for real S3 test")
    assume(s3Client.isDefined, "S3Client required for test")
    assume(multipartUploader.isDefined, "S3MultipartUploader required for test")

    val uploader = multipartUploader.get
    val content = new Array[Byte](50 * 1024 * 1024) // 50MB - below default 100MB threshold

    // Fill with random data to make upload realistic
    val random = new Random(12345) // Fixed seed for reproducibility
    random.nextBytes(content)

    val key = s"$testKeyPrefix/small-file-test.dat"

    println(s"ðŸ“¤ Uploading small file (${content.length / (1024 * 1024)}MB) to s3://$S3_BUCKET/$key")

    val result = uploader.uploadFile(S3_BUCKET, key, content)

    println(s"âœ… Upload completed: strategy=${result.strategy}, parts=${result.partCount}, size=${result.totalSize}")

    // Verify single-part upload was used
    assert(result.strategy == "single-part", s"Expected single-part upload, got: ${result.strategy}")
    assert(result.partCount == 1, s"Expected 1 part, got: ${result.partCount}")
    assert(result.totalSize == content.length, s"Expected size ${content.length}, got: ${result.totalSize}")
    assert(result.etag.nonEmpty, "ETag should not be empty")
    assert(result.uploadId.isEmpty, "Upload ID should be empty for single-part upload")

    // Verify the object exists in S3
    val headRequest = HeadObjectRequest.builder()
      .bucket(S3_BUCKET)
      .key(key)
      .build()

    val headResponse = s3Client.get.headObject(headRequest)
    assert(headResponse.contentLength() == content.length, s"S3 object size mismatch")

    println(s"âœ… Verified object exists in S3 with correct size: ${headResponse.contentLength()} bytes")
  }

  test("Real S3: Large files should use multipart upload") {
    assume(awsCredentials.isDefined, "AWS credentials required for real S3 test")
    assume(s3Client.isDefined, "S3Client required for test")
    assume(multipartUploader.isDefined, "S3MultipartUploader required for test")

    val uploader = multipartUploader.get
    val content = new Array[Byte](150 * 1024 * 1024) // 150MB - above default 100MB threshold

    // Fill with random data
    val random = new Random(54321) // Fixed seed for reproducibility
    random.nextBytes(content)

    val key = s"$testKeyPrefix/large-file-test.dat"

    println(s"ðŸ“¤ Uploading large file (${content.length / (1024 * 1024)}MB) to s3://$S3_BUCKET/$key")

    val startTime = System.currentTimeMillis()
    val result = uploader.uploadFile(S3_BUCKET, key, content)
    val uploadTime = System.currentTimeMillis() - startTime

    println(s"âœ… Upload completed in ${uploadTime}ms: strategy=${result.strategy}, parts=${result.partCount}, size=${result.totalSize}")

    // Verify multipart upload was used
    assert(result.strategy == "multipart", s"Expected multipart upload, got: ${result.strategy}")
    assert(result.partCount > 1, s"Expected multiple parts, got: ${result.partCount}")
    assert(result.totalSize == content.length, s"Expected size ${content.length}, got: ${result.totalSize}")
    assert(result.etag.nonEmpty, "ETag should not be empty")
    assert(result.uploadId.isDefined, "Upload ID should be defined for multipart upload")

    // Expected parts: 150MB / 64MB (default part size) = 3 parts (2 full + 1 partial)
    val expectedParts = math.ceil(content.length.toDouble / (64 * 1024 * 1024)).toInt
    assert(result.partCount == expectedParts, s"Expected $expectedParts parts, got: ${result.partCount}")

    // Verify the object exists in S3 with correct size
    val headRequest = HeadObjectRequest.builder()
      .bucket(S3_BUCKET)
      .key(key)
      .build()

    val headResponse = s3Client.get.headObject(headRequest)
    assert(headResponse.contentLength() == content.length, s"S3 object size mismatch")

    println(s"âœ… Verified object exists in S3 with correct size: ${headResponse.contentLength()} bytes")
  }

  test("Real S3: Custom configuration should be respected") {
    assume(awsCredentials.isDefined, "AWS credentials required for real S3 test")
    assume(s3Client.isDefined, "S3Client required for test")

    val config = S3MultipartConfig(
      multipartThreshold = 50L * 1024 * 1024, // 50MB threshold (lower than default)
      partSize = 32L * 1024 * 1024,           // 32MB parts (smaller than default 64MB)
      maxConcurrency = 2                       // 2 parallel uploads
    )

    val customUploader = new S3MultipartUploader(s3Client.get, config)

    try {
      val content = new Array[Byte](80 * 1024 * 1024) // 80MB - above custom 50MB threshold

      // Fill with random data
      val random = new Random(98765)
      random.nextBytes(content)

      val key = s"$testKeyPrefix/custom-config-test.dat"

      println(s"ðŸ“¤ Uploading file with custom config (${content.length / (1024 * 1024)}MB) to s3://$S3_BUCKET/$key")
      println(s"   Custom threshold: ${config.multipartThreshold / (1024 * 1024)}MB")
      println(s"   Custom part size: ${config.partSize / (1024 * 1024)}MB")
      println(s"   Custom concurrency: ${config.maxConcurrency}")

      val result = customUploader.uploadFile(S3_BUCKET, key, content)

      println(s"âœ… Upload completed: strategy=${result.strategy}, parts=${result.partCount}, size=${result.totalSize}")

      // With 80MB file and 32MB part size, should have 3 parts (2 full + 1 partial)
      val expectedParts = math.ceil(content.length.toDouble / config.partSize).toInt
      assert(result.partCount == expectedParts, s"Expected $expectedParts parts with custom config, got: ${result.partCount}")
      assert(result.strategy == "multipart", "Should use multipart with custom threshold")

      // Verify object in S3
      val headRequest = HeadObjectRequest.builder()
        .bucket(S3_BUCKET)
        .key(key)
        .build()

      val headResponse = s3Client.get.headObject(headRequest)
      assert(headResponse.contentLength() == content.length, "S3 object size mismatch")

      println(s"âœ… Custom configuration test passed with ${result.partCount} parts")

    } finally {
      customUploader.shutdown()
    }
  }

  test("Real S3: Streaming upload should work with input stream") {
    assume(awsCredentials.isDefined, "AWS credentials required for real S3 test")
    assume(s3Client.isDefined, "S3Client required for test")
    assume(multipartUploader.isDefined, "S3MultipartUploader required for test")

    val uploader = multipartUploader.get
    val content = new Array[Byte](120 * 1024 * 1024) // 120MB - above 100MB threshold for multipart

    // Fill with random data
    val random = new Random(11111)
    random.nextBytes(content)

    val inputStream = new ByteArrayInputStream(content)
    val key = s"$testKeyPrefix/streaming-upload-test.dat"

    println(s"ðŸ“¤ Uploading via stream (${content.length / (1024 * 1024)}MB) to s3://$S3_BUCKET/$key")

    val result = uploader.uploadStream(S3_BUCKET, key, inputStream, Some(content.length.toLong))

    println(s"âœ… Stream upload completed: strategy=${result.strategy}, parts=${result.partCount}, size=${result.totalSize}")

    // Should use multipart-stream strategy
    assert(result.strategy == "multipart-stream", s"Expected multipart-stream, got: ${result.strategy}")
    assert(result.totalSize == content.length, s"Expected size ${content.length}, got: ${result.totalSize}")
    assert(result.partCount > 0, "Should have at least one part")

    // Verify object in S3
    val headRequest = HeadObjectRequest.builder()
      .bucket(S3_BUCKET)
      .key(key)
      .build()

    val headResponse = s3Client.get.headObject(headRequest)
    assert(headResponse.contentLength() == content.length, "S3 object size mismatch")

    println(s"âœ… Streaming upload verified in S3 with correct size: ${headResponse.contentLength()} bytes")
  }

  test("Real S3: Upload performance with large merged splits config") {
    assume(awsCredentials.isDefined, "AWS credentials required for real S3 test")
    assume(s3Client.isDefined, "S3Client required for test")

    val config = S3MultipartConfig.forLargeMergedSplits
    val performanceUploader = new S3MultipartUploader(s3Client.get, config)

    try {
      val content = new Array[Byte](300 * 1024 * 1024) // 300MB - large merge scenario

      // Fill with random data
      val random = new Random(77777)
      random.nextBytes(content)

      val key = s"$testKeyPrefix/large-merge-performance-test.dat"

      println(s"ðŸ“¤ Performance test: uploading large file (${content.length / (1024 * 1024)}MB) to s3://$S3_BUCKET/$key")
      println(s"   Using forLargeMergedSplits config:")
      println(s"   - Part size: ${config.partSize / (1024 * 1024)}MB")
      println(s"   - Threshold: ${config.multipartThreshold / (1024 * 1024)}MB")
      println(s"   - Concurrency: ${config.maxConcurrency}")
      println(s"   - Max retries: ${config.maxRetries}")

      val startTime = System.currentTimeMillis()
      val result = performanceUploader.uploadFile(S3_BUCKET, key, content)
      val uploadTime = System.currentTimeMillis() - startTime

      val throughputMBps = (content.length.toDouble / (1024 * 1024)) / (uploadTime / 1000.0)

      println(f"âœ… Performance upload completed in ${uploadTime}ms ($throughputMBps%.2f MB/s)")
      println(s"   Strategy: ${result.strategy}, Parts: ${result.partCount}, Size: ${result.totalSize}")

      // Verify performance characteristics
      assert(result.strategy == "multipart", "Should use multipart for large files")
      assert(uploadTime > 0, "Upload should take measurable time")
      assert(throughputMBps > 0, "Should have positive throughput")

      // With 300MB file and 128MB part size (forLargeMergedSplits), should have 3 parts
      val expectedParts = math.ceil(content.length.toDouble / config.partSize).toInt
      assert(result.partCount == expectedParts, s"Expected $expectedParts parts, got: ${result.partCount}")

      // Verify object in S3
      val headRequest = HeadObjectRequest.builder()
        .bucket(S3_BUCKET)
        .key(key)
        .build()

      val headResponse = s3Client.get.headObject(headRequest)
      assert(headResponse.contentLength() == content.length, "S3 object size mismatch")

      println(f"âœ… Performance test verified: throughput $throughputMBps%.2f MB/s")

    } finally {
      performanceUploader.shutdown()
    }
  }
}