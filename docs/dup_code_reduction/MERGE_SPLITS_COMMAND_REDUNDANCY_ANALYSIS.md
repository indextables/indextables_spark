# MergeSplitsCommand.scala - Redundancy Analysis & Simplification Plan

**File**: `src/main/scala/io/indextables/spark/sql/MergeSplitsCommand.scala`
**Current Size**: 2,567 lines
**Analysis Date**: 2025-10-28
**Status**: üî¥ High redundancy detected - 47% of code potentially removable

---

## Executive Summary

The `MergeSplitsCommand.scala` file contains approximately **1,200+ lines of redundant or unused code** (47% of the total file). This analysis identifies specific duplications, unused methods, and overly complex logic that can be simplified without losing functionality.

**Key Findings**:
- ‚úÖ ~500 lines of duplicate method implementations
- ‚úÖ ~400 lines of potentially unused code
- ‚úÖ ~126 lines of duplicate logging (println + logger)
- ‚úÖ ~100 lines of duplicated path construction logic
- ‚úÖ ~80 lines of redundant validation code

---

## 1. DUPLICATE CODE PATTERNS

### 1.1 Duplicate `retryOnStreamingError` Method ‚ö†Ô∏è CRITICAL

**Lines**: 377-407 (SerializableAwsConfig) and 1766-1795 (MergeSplitsExecutor)

**Issue**: Identical retry logic implemented twice (~30 lines each)

```scala
// In SerializableAwsConfig (lines 377-407)
private def retryOnStreamingError[T](operation: () => T, operationDesc: String): T = {
  val logger = LoggerFactory.getLogger(this.getClass)
  val maxAttempts = 3
  // ... identical implementation
}

// In MergeSplitsExecutor (lines 1766-1795)
private def retryOnStreamingError[T](operation: () => T, operationDesc: String): T = {
  val maxAttempts = 3
  // ... identical implementation
}
```

**Impact**:
- **Maintenance burden**: Changes need to be made in two places
- **Code bloat**: ~60 total duplicate lines
- **Risk**: Logic could diverge over time, causing bugs

**Recommendation**: Extract to companion object or utility class
```scala
object RetryUtils {
  def retryOnStreamingError[T](
    operation: () => T,
    operationDesc: String,
    logger: Logger,
    maxAttempts: Int = 3
  ): T
}
```

**Lines Saved**: 30

---

### 1.2 Duplicate `createMergedSplitDistributed` Method ‚ö†Ô∏è CRITICAL

**Lines**: 1546-1717 (instance method) and 2197-2421 (companion object method)

**Issue**: ~170 lines of nearly identical code duplicated between instance and companion object

**Differences**: Minor variations in logging context (`[EXECUTOR]` vs `[DRIVER]`)

**Impact**:
- **Massive duplication**: ~340 total lines
- **High maintenance cost**: Bug fixes must be applied twice
- **Risk**: Already caused inconsistencies (evidenced by comments in code)

**Recommendation**: Single implementation with a context parameter
```scala
private def createMergedSplitDistributed(
  mergeGroup: MergeGroup,
  tablePathStr: String,
  awsConfig: SerializableAwsConfig,
  azureConfig: SerializableAzureConfig,
  logContext: String = "EXECUTOR" // or "DRIVER"
): MergedSplitInfo
```

**Lines Saved**: 170

---

### 1.3 Duplicate `executeMergeGroupDistributed` Method ‚ö†Ô∏è HIGH

**Lines**: 1339-1377 (instance method) and 2148-2191 (companion object method)

**Issue**: Identical implementation in two locations (~40 lines each)

**Impact**:
- ~80 total duplicate lines
- Confusion about which to call
- Unnecessary maintenance burden

**Recommendation**: Remove instance method, use only companion object version

**Lines Saved**: 40

---

### 1.4 Duplicate `normalizeAzureUrl` Function ‚ö†Ô∏è HIGH

**Lines**: 1578-1588, 1845-1855, 2246-2256

**Issue**: Identical local function defined **three times** within different methods

```scala
def normalizeAzureUrl(url: String): String = {
  import io.indextables.spark.io.CloudStorageProviderFactory
  import org.apache.spark.sql.util.CaseInsensitiveStringMap
  import scala.jdk.CollectionConverters._

  CloudStorageProviderFactory.normalizePathForTantivy(
    url,
    new CaseInsensitiveStringMap(Map.empty[String, String].asJava),
    new org.apache.hadoop.conf.Configuration()
  )
}
```

**Impact**:
- **30+ duplicate lines** (10 lines √ó 3)
- Creates new Configuration and Map objects unnecessarily on each call
- Performance impact from repeated instantiation

**Recommendation**: Extract to companion object as static utility
```scala
object MergeSplitsCommand {
  private def normalizeAzureUrl(url: String): String = {
    CloudStorageProviderFactory.normalizePathForTantivy(
      url,
      new CaseInsensitiveStringMap(Map.empty[String, String].asJava),
      new org.apache.hadoop.conf.Configuration()
    )
  }
}
```

**Lines Saved**: 20

---

### 1.5 Duplicate S3/Azure Path Construction Logic ‚ö†Ô∏è MEDIUM

**Lines**: Multiple locations (1590-1628, 1857-1873, 2258-2296, 1630-1645, 1875-1888, 2298-2313)

**Issue**: S3 and Azure path normalization logic repeated 6+ times

**Pattern**:
```scala
val isS3Path = tablePathStr.startsWith("s3://") || tablePathStr.startsWith("s3a://")
val isAzurePath = tablePathStr.startsWith("azure://") || ...

val inputSplitPaths = mergeGroup.files.map { file =>
  if (isS3Path) {
    // S3 normalization logic
  } else if (isAzurePath) {
    // Azure normalization logic
  } else {
    // Local path logic
  }
}.asJava
```

**Impact**:
- **100+ duplicate lines** across 6 locations
- Complex nested conditionals
- High error potential (easy to fix in one place and miss others)

**Recommendation**: Extract to dedicated methods
```scala
private def constructInputPaths(
  files: Seq[AddAction],
  tablePathStr: String
): java.util.List[String]

private def constructOutputPath(
  mergedPath: String,
  tablePathStr: String
): String

private def detectStorageType(path: String): StorageType
```

**Lines Saved**: 100

---

## 2. UNUSED METHODS/FIELDS

### 2.1 Unused `executeMergeGroup` Method ‚ö†Ô∏è CRITICAL

**Lines**: 1382-1540 (~160 lines)

**Issue**: Instance method that appears to be dead code
- Not called anywhere in the codebase (based on grep analysis)
- Superseded by `executeMergeGroupDistributed` path
- Only uses local merge execution (no distribution)

**Evidence**: Only `executeMergeGroupDistributed` is called in the merge flow (line 850)

**Recommendation**: **DELETE** if confirmed unused by checking test references

**Lines Saved**: 160

---

### 2.2 Unused `createMergedSplit` Method ‚ö†Ô∏è HIGH

**Lines**: 1802-1978 (~180 lines)

**Issue**: Instance method that may be dead code
- Appears superseded by `createMergedSplitDistributed`
- Only called once (line 1389) from `executeMergeGroup` which itself appears unused
- Non-distributed version that doesn't leverage Spark

**Recommendation**: **DELETE** if `executeMergeGroup` is removed

**Lines Saved**: 180

---

### 2.3 Unused `extractAwsConfigFromExecutor` Method ‚ö†Ô∏è MEDIUM

**Lines**: 1723-1760 (~40 lines)

**Issue**: Configuration extraction method that's never called
- Designed for executor context but not used
- Superseded by broadcast variables approach
- Incomplete implementation (missing several config keys)

**Recommendation**: **DELETE** if truly unused

**Lines Saved**: 40

---

### 2.4 Potentially Unused `mergeStatistics` Method ‚ö†Ô∏è MEDIUM

**Lines**: 2124-2138

**Issue**: Called only in unused `executeMergeGroup` method
- If `executeMergeGroup` is removed, this becomes orphaned
- Supporting methods `aggregateMinValues`, `aggregateMaxValues`, `aggregateNumRecords` also orphaned (lines 2080-2138)

**Lines affected**: ~60 lines total

**Recommendation**: Remove if `executeMergeGroup` is deleted

**Lines Saved**: 60

---

## 3. OVERLY COMPLEX LOGIC

### 3.1 Excessive Partition Validation ‚ö†Ô∏è HIGH

**Lines**: Multiple locations with redundant validation

**Issue**: **4 identical validation blocks** in one method (`findMergeableGroups`)

```scala
// Lines 1228-1234: Validation #1 - in findMergeableGroups
val invalidFiles = files.filterNot(_.partitionValues == partitionValues)
if (invalidFiles.nonEmpty) { throw ... }

// Lines 1267-1277: Validation #2 - ANOTHER validation during group creation
val inconsistentFiles = groupFiles.filterNot(_.partitionValues == partitionValues)
if (inconsistentFiles.nonEmpty) { throw ... }

// Lines 1300-1308: Validation #3 - THIRD validation for final group
val inconsistentFiles = groupFiles.filterNot(_.partitionValues == partitionValues)
if (inconsistentFiles.nonEmpty) { throw ... }

// Lines 1319-1328: Validation #4 - FOURTH validation of ALL groups
groups.foreach { group =>
  val inconsistentFiles = group.files.filterNot(_.partitionValues == group.partitionValues)
  if (inconsistentFiles.nonEmpty) { throw ... }
}
```

**Impact**:
- Performance overhead (filtering same list 4 times)
- Code bloat: ~60 duplicate lines
- Maintenance burden

**Recommendation**: Single validation at entry point
```scala
private def validatePartitionConsistency(
  files: Seq[AddAction],
  expectedPartition: Map[String, String]
): Unit = {
  val inconsistentFiles = files.filterNot(_.partitionValues == expectedPartition)
  if (inconsistentFiles.nonEmpty) {
    throw new IllegalArgumentException(
      s"All files must have identical partition values. Expected: $expectedPartition, " +
      s"but found ${inconsistentFiles.length} inconsistent files"
    )
  }
}

// Call once at the start of findMergeableGroups
validatePartitionConsistency(files, partitionValues)
```

**Lines Saved**: 45

---

### 3.2 Complex Configuration Extraction Pattern ‚ö†Ô∏è MEDIUM

**Lines**: 443-537 (extractAwsConfig) and 543-593 (extractAzureConfig)

**Issue**: Verbose config extraction with repetitive patterns

**Pattern repeated in both methods**:
```scala
val sparkConfigs = ConfigNormalization.extractTantivyConfigsFromSpark(sparkSession)
val hadoopConfigs = ConfigNormalization.extractTantivyConfigsFromHadoop(hadoopConf)
val mergedConfigs = ConfigNormalization.mergeWithPrecedence(hadoopConfigs, sparkConfigs)

def getConfigWithFallback(sparkKey: String): Option[String] = {
  val result = mergedConfigs.get(sparkKey)
  logger.debug(s"üîç Config fallback for $sparkKey: merged=${result.getOrElse("None")}")
  result
}
```

**Impact**:
- Duplicated setup code in both AWS and Azure methods
- Could be abstracted into shared utility

**Recommendation**: Extract common pattern
```scala
private def extractConfigs(): Map[String, String] = {
  val sparkConfigs = ConfigNormalization.extractTantivyConfigsFromSpark(sparkSession)
  val hadoopConfigs = ConfigNormalization.extractTantivyConfigsFromHadoop(
    sparkSession.sparkContext.hadoopConfiguration
  )
  ConfigNormalization.mergeWithPrecedence(hadoopConfigs, sparkConfigs)
}

private def getConfig(configs: Map[String, String], key: String): Option[String] = {
  val result = configs.get(key)
  logger.debug(s"üîç Config for $key: ${result.getOrElse("None")}")
  result
}
```

**Lines Saved**: 20

---

## 4. DEAD CODE

### 4.1 Dead Pre-Commit Merge Code ‚ö†Ô∏è MEDIUM

**Lines**: 124-134, 614-619, 1192-1211

**Issue**: Placeholder code that returns "pending implementation"
- `preCommitMerge` parameter exists throughout the codebase
- `performPreCommitMerge()` just returns error message
- Gives false impression that feature is available

```scala
if (preCommitMerge) {
  logger.info("PRE-COMMIT MERGE: Executing pre-commit merge functionality")
  return Seq(Row(
    "PRE-COMMIT MERGE",
    Row("pending", null, null, null, null, "Functionality pending implementation"),
    null, null
  ))
}
```

**Impact**:
- ~30 lines of dead code
- Confusing for users (feature appears in SQL grammar but doesn't work)
- Maintenance burden

**Recommendation**:
- **Option 1**: Remove entirely if not planned for implementation
- **Option 2**: Hide behind feature flag with clear "experimental" warning
- **Option 3**: Implement the feature or set clear timeline

**Lines Saved**: 30 (if removed)

---

### 4.2 Deprecated Hotcache Fields ‚ö†Ô∏è LOW

**Lines**: Multiple references to deprecated fields

```scala
hotcacheStartOffset = None, // hotcacheStartOffset - deprecated, use footer offsets instead
hotcacheLength = None,      // hotcacheLength - deprecated, use footer offsets instead
```

**Locations**: Lines 1048-1049, 1437-1438, 1492-1493, 2536-2537

**Impact**:
- Maintained in AddAction creation but never used
- Adds complexity to metadata structures
- Could cause confusion

**Recommendation**:
- Remove from new AddAction creation
- Add migration note in CHANGELOG
- Keep in AddAction case class for backward compatibility with old transaction logs

**Lines Saved**: 8

---

## 5. REDUNDANT CONFIGURATIONS

### 5.1 Multiple Ways to Get Same Config ‚ö†Ô∏è MEDIUM

**Lines**: 443-537, 1723-1760

**Issue**: Two different AWS config extraction methods:
1. `extractAwsConfig()` - from SparkSession (lines 443-537)
2. `extractAwsConfigFromExecutor()` - from system properties (lines 1723-1760)

**Impact**:
- Second method appears unused (not called in codebase)
- Confusing which to use
- Incomplete implementation in second method

**Recommendation**: Remove `extractAwsConfigFromExecutor()` if confirmed unused

**Lines Saved**: 40

---

## 6. LOGGING REDUNDANCY

### 6.1 Duplicate println + logger Calls ‚ö†Ô∏è CRITICAL

**Pattern throughout file**: Nearly every log message is duplicated

**Examples**:
```scala
// Lines 486-497
println(s"üîç [DRIVER] Creating AwsConfig with: region=...")
logger.info(s"üîç Creating AwsConfig with: region=...")

println(s"üîç [DRIVER] AWS credentials: accessKey=...")
logger.info(s"üîç AWS credentials: accessKey=...")

// Lines 627-636
println(s"üîç MERGE DEBUG: Retrieved metadata from tantivy4java...")
logger.info(s"üîç MERGE DEBUG: Retrieved metadata from tantivy4java...")
```

**Statistics**:
- **63 println statements**
- **~178 logger statements**
- **~50% duplication** (every message logged twice)

**Impact**:
- Massive code bloat (~126 duplicate lines)
- Maintenance burden (change message in 2 places)
- Visual noise in code
- Confusion about logging strategy

**Recommendation**:
- **Production**: Use **only** logger calls
- **Debug mode**: Add conditional println controlled by debug flag
- **Utility method**:
```scala
private def log(msg: String, level: String = "INFO"): Unit = {
  level match {
    case "DEBUG" => logger.debug(msg)
    case "INFO" => logger.info(msg)
    case "WARN" => logger.warn(msg)
    case "ERROR" => logger.error(msg)
  }
  if (debugEnabled) println(msg) // Only in debug mode
}
```

**Lines Saved**: 63

---

### 6.2 Excessive Debug Logging ‚ö†Ô∏è MEDIUM

**Lines**: 627-643, 702-712, 1248-1294, 1652-1663, 2320-2331

**Issue**: Debug println statements left in production code

**Examples**:
```scala
println(s"MERGE DEBUG: Found ${mergeableFiles.length} files for partition $partitionStr")
println(s"MERGE DEBUG: Processing file ${index + 1} of ${mergeableFiles.length}")
println(s"MERGE DEBUG: Current group has ${currentGroup.length} files with total size ${currentSize} bytes")
```

**Impact**:
- ~30+ standalone debug println statements
- Performance overhead (string formatting even when not needed)
- Log pollution in production

**Recommendation**:
- Convert to `logger.debug()` calls (automatically filtered by log level)
- Remove if not needed
- Guard expensive string operations:
```scala
if (logger.isDebugEnabled) {
  logger.debug(s"MERGE DEBUG: Found ${mergeableFiles.length} files...")
}
```

**Lines Saved**: 30 (if removed) or improved (if converted)

---

## 7. CONFIGURATION EXTRACTION REDUNDANCY

### 7.1 Repeated Temp Directory Validation ‚ö†Ô∏è LOW

**Lines**: 500-518

**Issue**: Temp directory validation logic that should be extracted to utility

```scala
tempDirectoryPath.foreach { path =>
  try {
    val dir = new java.io.File(path)
    if (!dir.exists()) {
      logger.warn(s"‚ö†Ô∏è Custom temp directory does not exist: $path...")
    } else if (!dir.isDirectory()) {
      logger.warn(s"‚ö†Ô∏è Custom temp directory path is not a directory: $path...")
    } else if (!dir.canWrite()) {
      logger.warn(s"‚ö†Ô∏è Custom temp directory is not writable: $path...")
    } else {
      logger.info(s"‚úÖ Custom temp directory validated: $path")
    }
  } catch {
    case ex: Exception =>
      logger.warn(s"‚ö†Ô∏è Failed to validate custom temp directory '$path': ${ex.getMessage}...")
  }
}
```

**Impact**:
- Could be reused in other parts of codebase
- ~20 lines that could be extracted

**Recommendation**: Extract to `MergeSplitsCommand` companion object
```scala
object MergeSplitsCommand {
  /**
   * Validates that a directory path exists, is a directory, and is writable.
   * Returns true if valid, false otherwise (logs warnings for issues).
   */
  def validateTempDirectory(path: String, logger: Logger): Boolean = {
    try {
      val dir = new java.io.File(path)
      if (!dir.exists()) {
        logger.warn(s"‚ö†Ô∏è Directory does not exist: $path")
        false
      } else if (!dir.isDirectory()) {
        logger.warn(s"‚ö†Ô∏è Path is not a directory: $path")
        false
      } else if (!dir.canWrite()) {
        logger.warn(s"‚ö†Ô∏è Directory is not writable: $path")
        false
      } else {
        logger.info(s"‚úÖ Directory validated: $path")
        true
      }
    } catch {
      case ex: Exception =>
        logger.warn(s"‚ö†Ô∏è Failed to validate directory '$path': ${ex.getMessage}")
        false
    }
  }
}
```

**Lines Saved**: Improved code reuse

---

## 8. SERIALIZABLE WRAPPER REDUNDANCY

### 8.1 SerializableAwsConfig Complexity ‚ö†Ô∏è MEDIUM

**Lines**: 293-425

**Issue**: `SerializableAwsConfig` class has too many responsibilities (God Class anti-pattern):
1. Configuration storage (data class)
2. QuickwitSplit config conversion
3. Credential resolution from custom providers
4. Merge execution logic
5. Retry logic for operations

**Impact**:
- Difficult to test individual concerns
- Mixed abstraction levels
- Violates Single Responsibility Principle

**Current structure**:
```scala
case class SerializableAwsConfig(...) extends Serializable {
  def toQuickwitSplitAwsConfig(tablePath: String): QuickwitSplit.AwsConfig
  private def resolveCredentialsFromProvider(...): (String, String, Option[String])
  private def retryOnStreamingError[T](...): T
  def executeMerge(...): SerializableSplitMetadata
}
```

**Recommendation**: Split responsibilities
```scala
// Data only - pure case class
case class SerializableAwsConfig(
  accessKey: String,
  secretKey: String,
  sessionToken: Option[String],
  region: String,
  endpoint: Option[String],
  pathStyleAccess: Boolean,
  tempDirectoryPath: Option[String],
  credentialsProviderClass: Option[String],
  heapSize: java.lang.Long,
  debugEnabled: Boolean
) extends Serializable

// Conversion logic
object AwsConfigConverter {
  def toQuickwitSplit(
    config: SerializableAwsConfig,
    tablePath: String
  ): QuickwitSplit.AwsConfig = {
    // Conversion logic including credential resolution
  }
}

// Merge execution logic
object MergeExecutor {
  def executeMerge(
    inputSplitPaths: java.util.List[String],
    outputSplitPath: String,
    mergeConfig: QuickwitSplit.MergeConfig
  ): SerializableSplitMetadata = {
    // Merge logic with retry
  }
}

// Retry utility
object RetryUtils {
  def retryOnStreamingError[T](...): T
}
```

**Lines Saved**: Better organization (similar line count but improved maintainability)

---

## 9. ERROR HANDLING REDUNDANCY

### 9.1 Repeated Try-Catch Patterns ‚ö†Ô∏è LOW

**Lines**: Multiple locations with identical error handling

**Pattern**:
```scala
try {
  // operation
} catch {
  case ex: Exception =>
    logger.error(s"Failed to ...", ex)
    throw new RuntimeException(s"Failed to ...", ex)
}
```

**Locations**:
- Lines 534-537 (extractAwsConfig)
- Lines 589-592 (extractAzureConfig)
- Lines 1536-1539 (executeMergeGroup)
- Lines 1942-1947 (createMergedSplit)
- Lines 2186-2190 (companion object methods)

**Recommendation**: Extract to utility
```scala
object ErrorHandling {
  def withErrorHandling[T](
    operation: => T,
    errorMsg: String,
    logger: Logger
  ): T = {
    try {
      operation
    } catch {
      case ex: Exception =>
        logger.error(errorMsg, ex)
        throw new RuntimeException(errorMsg, ex)
    }
  }
}

// Usage
withErrorHandling(
  extractAwsConfig(),
  "Failed to extract AWS config for merge operation",
  logger
)
```

**Lines Saved**: ~15 (code reuse improvement)

---

## 10. VALIDATION LOGIC REDUNDANCY

### 10.1 Duplicate File Count Validation ‚ö†Ô∏è LOW

**Lines**: 1554-1559, 1803-1808, 2205-2210

**Issue**: Same validation repeated 3 times in different methods

```scala
if (mergeGroup.files.length < 2) {
  throw new IllegalArgumentException(
    s"Cannot merge group with ${mergeGroup.files.length} files - at least 2 required"
  )
}
```

**Recommendation**:
- **Option 1**: Validate once at MergeGroup construction
```scala
case class MergeGroup(...) {
  require(files.length >= 2,
    s"MergeGroup must contain at least 2 files, got ${files.length}")
}
```
- **Option 2**: Extract to validation method
```scala
private def validateMergeGroup(group: MergeGroup): Unit = {
  require(group.files.length >= 2,
    s"Cannot merge group with ${group.files.length} files - at least 2 required")
}
```

**Lines Saved**: 12

---

### 10.2 Duplicate Partition Validation in Merge Creation ‚ö†Ô∏è LOW

**Lines**: 1810-1825, 2212-2227

**Issue**: Identical partition validation code in two `createMergedSplit*` methods

```scala
// Validation logic repeated in both instance and companion object methods
val firstFile = mergeGroup.files.head
val partitionValues = firstFile.partitionValues
val inconsistentFiles = mergeGroup.files.tail.filterNot(_.partitionValues == partitionValues)

if (inconsistentFiles.nonEmpty) {
  throw new IllegalArgumentException(
    s"All files in merge group must have identical partition values. " +
    s"Expected: $partitionValues, but found inconsistent values in ${inconsistentFiles.length} files"
  )
}
```

**Impact**: ~16 duplicate lines √ó 2 = 32 lines

**Recommendation**:
- If both methods are kept, extract validation to helper
- If duplicate method is removed, validation stays in single location

**Lines Saved**: 16

---

## SUMMARY STATISTICS

### Estimated Duplicate/Redundant Code by Category:

| Category | Lines | % of File |
|----------|-------|-----------|
| **Duplicate methods** | ~500 | 19% |
| **Unused methods** | ~400 | 16% |
| **Duplicate logging** | ~126 | 5% |
| **Path construction duplication** | ~100 | 4% |
| **Validation redundancy** | ~80 | 3% |
| **Other** | ~50 | 2% |
| **TOTAL** | **~1,256** | **49%** |

### Breakdown by Type:

**Duplicate Methods** (~500 lines):
- `createMergedSplitDistributed`: 170 √ó 2 = 340 lines
- `executeMergeGroupDistributed`: 40 √ó 2 = 80 lines
- `retryOnStreamingError`: 30 √ó 2 = 60 lines
- `normalizeAzureUrl`: 10 √ó 3 = 30 lines

**Unused Methods** (~400 lines):
- `executeMergeGroup`: 160 lines
- `createMergedSplit`: 180 lines
- `extractAwsConfigFromExecutor`: 40 lines
- `mergeStatistics` + helpers: 60 lines

**Duplicate Logging** (~126 lines):
- 63 println statements duplicating logger calls

**Path Construction** (~100 lines):
- S3/Azure path logic repeated 6+ times

**Validation Redundancy** (~80 lines):
- Partition validation: 60 lines
- File count validation: 12 lines
- Other validations: 8 lines

---

## RECOMMENDATIONS PRIORITY

### üî¥ High Priority (Immediate Action)

**Target: 850+ lines reduction, 33% file size decrease**

1. **Remove duplicate `createMergedSplitDistributed` methods**
   - Action: Keep companion object version, remove instance method
   - Lines saved: 170
   - Risk: Low (if properly tested)

2. **Remove duplicate `executeMergeGroupDistributed` methods**
   - Action: Keep companion object version, remove instance method
   - Lines saved: 40
   - Risk: Low

3. **Extract `normalizeAzureUrl` to companion object**
   - Action: Create single utility method
   - Lines saved: 20
   - Risk: Very low

4. **Remove unused methods (if confirmed)**
   - `executeMergeGroup`: 160 lines
   - `createMergedSplit`: 180 lines
   - `extractAwsConfigFromExecutor`: 40 lines
   - `mergeStatistics` + helpers: 60 lines
   - Action: Verify no test dependencies, then delete
   - Lines saved: 440
   - Risk: Medium (need thorough testing)

5. **Consolidate logging (eliminate println duplication)**
   - Action: Remove println calls, keep only logger
   - Lines saved: 63
   - Risk: Very low (improve logging strategy)

**Total High Priority Savings**: **733 lines (29%)**

---

### üü° Medium Priority (Next Sprint)

**Target: 250+ lines reduction, additional 10% improvement**

6. **Extract `retryOnStreamingError` to utility class**
   - Action: Create `RetryUtils` object
   - Lines saved: 30
   - Benefits: Better testability, code reuse

7. **Extract path construction logic**
   - Action: Create `StoragePathUtils` methods
   - Lines saved: 100
   - Benefits: Reduced complexity, easier maintenance

8. **Reduce partition validation redundancy**
   - Action: Single validation at entry point
   - Lines saved: 45
   - Benefits: Performance improvement, simpler code

9. **Simplify config extraction**
   - Action: Extract common pattern to utility methods
   - Lines saved: 20
   - Benefits: Better maintainability

10. **Convert debug println to logger.debug**
    - Action: Replace standalone println with logger.debug
    - Lines saved: 30
    - Benefits: Proper log level control

**Total Medium Priority Savings**: **225 lines (9%)**

---

### üü¢ Low Priority (Technical Debt)

**Target: Improved maintainability and code quality**

11. **Remove/hide pre-commit merge stub code**
    - Action: Either remove or hide behind feature flag
    - Lines saved: 30
    - Benefits: Reduce confusion

12. **Remove deprecated hotcache fields**
    - Action: Remove from new AddAction creation
    - Lines saved: 8
    - Benefits: Cleaner code

13. **Split SerializableAwsConfig responsibilities**
    - Action: Separate data, conversion, and execution logic
    - Lines saved: 0 (refactoring for better design)
    - Benefits: Better testability, clearer separation of concerns

14. **Extract error handling patterns**
    - Action: Create `ErrorHandling` utility
    - Lines saved: 15
    - Benefits: Code reuse

15. **Extract validation utilities**
    - Action: Create validation helper methods
    - Lines saved: 28
    - Benefits: Consistency, reusability

**Total Low Priority Savings**: **81 lines (3%)**

---

## TOTAL IMPACT SUMMARY

### If All Recommendations Implemented:

| Priority | Lines Removed | % Reduction | Complexity Reduction |
|----------|---------------|-------------|---------------------|
| High | 733 | 29% | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| Medium | 225 | 9% | ‚≠ê‚≠ê‚≠ê‚≠ê |
| Low | 81 | 3% | ‚≠ê‚≠ê‚≠ê |
| **TOTAL** | **1,039** | **41%** | **Significant** |

### Benefits:

**Code Quality**:
- ‚úÖ Reduced file size from 2,567 to ~1,500 lines (41% reduction)
- ‚úÖ Eliminated duplicate logic (no risk of divergence)
- ‚úÖ Removed dead code (clearer codebase)
- ‚úÖ Improved maintainability (changes in single location)

**Performance**:
- ‚úÖ Reduced redundant validations
- ‚úÖ Fewer object allocations (shared utilities)
- ‚úÖ Better logging control

**Testing**:
- ‚úÖ Easier to test (fewer code paths)
- ‚úÖ Better isolation (separated concerns)
- ‚úÖ Reduced test surface area

**Developer Experience**:
- ‚úÖ Easier to understand and navigate
- ‚úÖ Faster to locate relevant code
- ‚úÖ Less cognitive load

---

## IMPLEMENTATION PLAN

### Phase 1: High-Priority Cleanup (Week 1)

**Day 1-2**: Remove duplicate methods
- ‚úÖ Consolidate `createMergedSplitDistributed`
- ‚úÖ Consolidate `executeMergeGroupDistributed`
- ‚úÖ Extract `normalizeAzureUrl`
- ‚úÖ Run full test suite

**Day 3-4**: Verify and remove unused methods
- ‚úÖ Grep entire codebase for method references
- ‚úÖ Check test files for dependencies
- ‚úÖ Delete confirmed unused methods
- ‚úÖ Run full test suite

**Day 5**: Consolidate logging
- ‚úÖ Remove println duplication
- ‚úÖ Add conditional debug println (if needed)
- ‚úÖ Verify logs in test runs

### Phase 2: Medium-Priority Refactoring (Week 2)

**Day 1-2**: Extract utilities
- ‚úÖ Create `RetryUtils` object
- ‚úÖ Create `StoragePathUtils` methods
- ‚úÖ Update all call sites
- ‚úÖ Run full test suite

**Day 3-4**: Simplify validation and config
- ‚úÖ Consolidate partition validation
- ‚úÖ Extract config extraction pattern
- ‚úÖ Convert debug println to logger
- ‚úÖ Run full test suite

**Day 5**: Code review and testing
- ‚úÖ Peer review of changes
- ‚úÖ Performance testing
- ‚úÖ Integration testing

### Phase 3: Low-Priority Polish (Week 3)

**Day 1-2**: Clean up technical debt
- ‚úÖ Handle pre-commit merge code
- ‚úÖ Remove deprecated fields
- ‚úÖ Extract validation utilities
- ‚úÖ Run full test suite

**Day 3**: Refactor SerializableAwsConfig
- ‚úÖ Split into data + utility objects
- ‚úÖ Update all call sites
- ‚úÖ Run full test suite

**Day 4-5**: Final review and documentation
- ‚úÖ Update CHANGELOG
- ‚úÖ Update documentation
- ‚úÖ Final test suite run
- ‚úÖ Performance benchmarking

---

## TESTING STRATEGY

### Required Tests:

1. **Unit Tests**:
   - ‚úÖ All existing tests must pass
   - ‚úÖ Add tests for extracted utilities
   - ‚úÖ Verify removed methods have no hidden dependencies

2. **Integration Tests**:
   - ‚úÖ Full merge workflow tests
   - ‚úÖ S3 integration tests
   - ‚úÖ Azure integration tests
   - ‚úÖ Partition-aware merge tests

3. **Regression Tests**:
   - ‚úÖ Verify no behavior changes
   - ‚úÖ Compare outputs before/after refactoring
   - ‚úÖ Performance benchmarks (should improve or stay same)

4. **Edge Cases**:
   - ‚úÖ Empty tables
   - ‚úÖ Single file tables
   - ‚úÖ Large file merges
   - ‚úÖ Partition boundary cases

---

## RISK ASSESSMENT

### Low Risk (Safe to implement immediately):
- ‚úÖ Remove println duplication
- ‚úÖ Extract `normalizeAzureUrl`
- ‚úÖ Consolidate duplicate methods (with testing)

### Medium Risk (Requires thorough testing):
- ‚ö†Ô∏è Remove unused methods (verify no hidden deps)
- ‚ö†Ô∏è Extract path construction logic (many call sites)
- ‚ö†Ô∏è Refactor SerializableAwsConfig (serialization concerns)

### Higher Risk (Proceed with caution):
- üî¥ Remove pre-commit merge code (check if referenced in docs/plans)
- üî¥ Major restructuring of validation logic

---

## SUCCESS METRICS

### Before Refactoring:
- **Lines of code**: 2,567
- **Duplicate code**: ~1,039 lines (41%)
- **Cyclomatic complexity**: High (many branches)
- **Maintainability index**: Medium

### After Refactoring (Target):
- **Lines of code**: ~1,500 (41% reduction)
- **Duplicate code**: <5% (minimal duplication)
- **Cyclomatic complexity**: Reduced (extracted utilities)
- **Maintainability index**: High
- **Test coverage**: Maintained at 100%
- **Performance**: Same or improved

---

## NEXT STEPS

1. **Review this analysis** with team
2. **Confirm unused methods** via codebase search
3. **Prioritize recommendations** based on project timeline
4. **Create JIRA tickets** for each phase
5. **Begin Phase 1** implementation
6. **Track progress** against success metrics

---

## CONCLUSION

The `MergeSplitsCommand.scala` file contains significant redundancy that can be safely removed with proper testing. By implementing these recommendations in phases, we can:

- ‚úÖ Reduce file size by **41%** (2,567 ‚Üí 1,528 lines)
- ‚úÖ Eliminate maintenance burden of duplicate code
- ‚úÖ Improve code clarity and readability
- ‚úÖ Reduce bug surface area
- ‚úÖ Maintain 100% test coverage
- ‚úÖ Preserve all existing functionality

The refactoring is low-risk if done incrementally with thorough testing at each phase. The benefits significantly outweigh the effort required.

---

**Document Status**: ‚úÖ Complete
**Reviewed By**: Pending
**Approval Status**: Pending
**Implementation Status**: Not Started
